I started off this project by looking at the overall data provided within the training dataset. Identified data types, blank values, numerical value's summary, and visually exploring the response variables. First I started with the relationship between numerical variables and the response variable as there aren’t many. Determined the best way to get a linear relationship would be by using transformations such as log and square root. Also, normalized year to prevent magnitude differences and converted zip-code to a categorical variable so I can look at specific regions (first number of the zipcode) rather than using each unique one. Lastly, turning rating and review count into an interactive variable so it weights rating per review count, and dropping LIstingID as it provides no significance in the model.
Next, cleaning the categorical variables took the longest time in this project. Majority of it required turning the columns into dummy variables through one-hot encoding but some required extensive cleaning such as columns ‘VehColorExt’, ‘VehColorInt’, ‘VehFeats’, and ‘VehEngine’ as I want to extract duplicated values and make one-hot coding as efficient as possible. Some categorical variables were also dropped as they provided no significance due to high number of unique values or only 1 unique value. Some variables also contained NA values but that was considered as a variable as I assume that could lead in price reduction.
After cleaning the data, I look at potential algorithms that I can use to build our model. Before applying the whole training dataset into one model, I decided to split the training data into training and validation to assess which algorithm works the best for predicting price and trim. For predicting price, I used four regression models: linear, lasso, ridge, and gradient boost. I believe these models are diverse enough to identify whether a simple, regularization or tuned model would work the best when predicting the price. Ultimately, the gradient boost had the best R-Squared value along with the least root mean squared error which is why I chose it to as our final model to predict on the test values.
For the second model, before applying any algorithms, I had to tweak the data as it was imbalanced. Given that some values within the vehicle trim column had less than 10 values, I decided to remove them. In addition, not all the data was evenly balanced as some values had ~2000 counts where as some only had ~20 so I decided to oversample the data using SMOTE. After oversampling, I ran the random forest and logistic regression algorithms and random forest achieved far better accuracy, precision, and recall than logistic regression. I also decided to tune the random forest model and although the accuracy remained stagnant, the precision and recall increased by 10 percent. Therefore, I went with the tuned random forest model.
